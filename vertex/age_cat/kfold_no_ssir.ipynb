{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff914e17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60102a25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idx2numpy in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (1.2.3)\r\n",
      "Requirement already satisfied: six in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from idx2numpy) (1.16.0)\r\n",
      "Requirement already satisfied: numpy in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from idx2numpy) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca7286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4c5f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadr in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (0.4.6)\r\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from pyreadr) (1.3.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.0->pyreadr) (2021.3)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.0->pyreadr) (1.20.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/ali/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd4146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random state\n",
    "random_state = 0\n",
    "\n",
    "# Import libraries, modules, and functions\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import idx2numpy\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d17414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA availability:  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display GPU/CUDA availability\n",
    "print('\\nCUDA availability:  %s\\n' % (torch.cuda.is_available()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ff73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/Users/ali/Desktop/Jul/apoe2_paper/vertex/age_cat/' # data path after downloading from mnist website abnd unziping in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e3a169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     0.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    1.0\n",
      "12    1.0\n",
      "13    1.0\n",
      "14    1.0\n",
      "15    1.0\n",
      "16    1.0\n",
      "17    1.0\n",
      "18    1.0\n",
      "19    1.0\n",
      "20    1.0\n",
      "21    0.0\n",
      "22    1.0\n",
      "23    1.0\n",
      "24    1.0\n",
      "25    1.0\n",
      "26    1.0\n",
      "27    1.0\n",
      "28    1.0\n",
      "29    1.0\n",
      "30    1.0\n",
      "31    1.0\n",
      "32    0.0\n",
      "Name: age_cat, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "image=pyreadr.read_r(data_path + 'image_all.rda')\n",
    "#print(image.keys())\n",
    "image = image[\"image_all\"]\n",
    "#print(image.shape)\n",
    "age_cat=pyreadr.read_r(data_path + 'response.rda')\n",
    "#print(sex.keys())\n",
    "age_cat= age_cat[\"response\"]\n",
    "#print(sex.shape)\n",
    "#print(sex)\n",
    "age_cat=age_cat[\"age_cat\"]\n",
    "print(age_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0de4216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    1.0\n",
       "12    1.0\n",
       "13    1.0\n",
       "14    1.0\n",
       "15    1.0\n",
       "16    1.0\n",
       "17    1.0\n",
       "18    1.0\n",
       "19    1.0\n",
       "20    1.0\n",
       "21    0.0\n",
       "22    1.0\n",
       "23    1.0\n",
       "24    1.0\n",
       "25    1.0\n",
       "26    1.0\n",
       "27    1.0\n",
       "28    1.0\n",
       "29    1.0\n",
       "30    1.0\n",
       "31    1.0\n",
       "32    0.0\n",
       "Name: age_cat, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_cat[age_cat==0]=0;\n",
    "age_cat[age_cat==1]=1;\n",
    "type(age_cat)\n",
    "age_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ec5d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train_npy = image #\n",
    "y_train_npy = age_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09694986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten images\n",
    "X_raw_train_npy =  np.reshape(X_raw_train_npy, newshape =(X_raw_train_npy.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5a8bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 49603)\n",
      "33\n",
      "(33,)\n"
     ]
    }
   ],
   "source": [
    "print(X_raw_train_npy.shape)\n",
    "print(X_raw_train_npy.shape[0])\n",
    "print(y_train_npy.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b9c9c",
   "metadata": {},
   "source": [
    "### store original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95a8035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train_npy=X_raw_train_npy.to_numpy()\n",
    "y_train_npy=y_train_npy.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b903d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train_npy_orig =  X_raw_train_npy\n",
    "y_train_npy_orig = y_train_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49349e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_npy = np.array(y_train_npy, dtype=int)\n",
    "y_train_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78575f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_npy_orig.shape \n",
    "\n",
    "type(y_train_npy_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc92c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "nfold=5\n",
    "kfold = KFold(nfold,  True )\n",
    "for train, test in kfold.split(X_raw_train_npy_orig):\n",
    "    print( type(X_raw_train_npy_orig[test].shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a69232b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (26, 49603), test: (7, 49603)\n",
      "train: (26,), test: (7,)\n",
      "train: (26, 49603), test: (7, 49603)\n",
      "train: (26,), test: (7,)\n",
      "train: (26, 49603), test: (7, 49603)\n",
      "train: (26,), test: (7,)\n",
      "train: (27, 49603), test: (6, 49603)\n",
      "train: (27,), test: (6,)\n",
      "train: (27, 49603), test: (6, 49603)\n",
      "train: (27,), test: (6,)\n"
     ]
    }
   ],
   "source": [
    "nfold=5\n",
    "kfold = KFold(nfold,  True )\n",
    "for train, test in kfold.split(X_raw_train_npy_orig):\n",
    "\tprint('train: %s, test: %s' % (X_raw_train_npy_orig[train].shape, X_raw_train_npy_orig[test].shape))\n",
    "\tprint('train: %s, test: %s' % (y_train_npy_orig[train].shape, y_train_npy_orig[test].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce9cb35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_npy_orig[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cb00ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST PyTorch dataset class definition\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset object for the MNIST dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Flattened image data of size (n_samples x 784).\n",
    "    y : torch.Tensor\n",
    "        Image labels, integer values 0, 1, 2, ..., 9.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Flattened image data of size (n_samples x 784).\n",
    "    y : torch.Tensor\n",
    "        Image labels, integer values 0, 1, 2, ..., 9.\n",
    "    len : int\n",
    "        Length of dataset/number of observations in dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # MNIST PyTorch dataset instantiation method\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        MNIST dataset instantiation method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            Flattened image data of size (n_samples x 784).\n",
    "        y : torch.Tensor\n",
    "            Image labels, integer values 0, 1, 2, ..., 9.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            Flattened image data of size (n_samples x 784).\n",
    "        y : torch.Tensor\n",
    "            Image labels, integer values 0, 1, 2, ..., 9.\n",
    "        len : int\n",
    "            Length of dataset/number of observations in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Assign attributes\n",
    "        self.X = X         # flattened image data\n",
    "        self.y = y         # image labels\n",
    "        self.len = len(X)  # length of dataset\n",
    "\n",
    "    # Dataset length method\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Method that returns length of dataset/number of observations in dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.len : int\n",
    "            Length of dataset/number of observations in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.len\n",
    "\n",
    "    # Dataset indexing method\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Method that retrieves samples and corresponding labels from the dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Integer index of dataset sample to be retrieved.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        X_samp : torch.Tensor\n",
    "            Image sample.\n",
    "        y_samp : torch.Tensor\n",
    "            Label corresponding to image sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve sample of data indexed by idx\n",
    "        X_samp = self.X[idx, :]\n",
    "        y_samp = self.y[idx]\n",
    "\n",
    "        # Return data sample\n",
    "        return X_samp, y_samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19d57a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model class definition\n",
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a logistic regression model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Size/dimensionality of the model input data.\n",
    "    output_dim : int\n",
    "        Size/dimensionality of the model output.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Size/dimensionality of the model input data.\n",
    "    output_dim : int\n",
    "        Size/dimensionality of the model output.\n",
    "    linear : torch.nn.Linear\n",
    "        Fully-connected/linear layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Logistic regression instantiation method\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Logistic regression model instantiation method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Size/dimensionality of the model input data.\n",
    "        output_dim : int\n",
    "            Size/dimensionality of the model output.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Size/dimensionality of the model input data.\n",
    "        output_dim : int\n",
    "            Size/dimensionality of the model output.\n",
    "        linear : torch.nn.Linear\n",
    "            Fully-connected/linear layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inherit from torch.nn.Module\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        # Assign logistic regression parameters to model attributes\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=input_dim,\n",
    "            out_features=output_dim,\n",
    "            bias=True)\n",
    "    \n",
    "    # Logistic regression forward pass method\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Logistic regression forward pass method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor()\n",
    "            Tensor of input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor()\n",
    "             Raw model predictions (not passed through sigmoid or softmax function).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate model predictions/logits\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        # Return model output\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "464eb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate cross-entropy loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5586bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs (epochs = # of passes through data)\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06605785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eac07a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 6.5717 | train acc.: 0.6154\n",
      "  E02 | train loss: 7.3870 | train acc.: 0.8846\n",
      "  E03 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "Logistic regression model training complete.\n",
      "\n",
      "Time to train logistic regression model:  0.3 s\n",
      "\n",
      "\n",
      "Logistic regression model MNIST test acc.:  0.8571\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 14.9529 | train acc.: 0.5385\n",
      "  E02 | train loss: 14.5446 | train acc.: 0.8846\n",
      "  E03 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "Logistic regression model training complete.\n",
      "\n",
      "Time to train logistic regression model:  0.3 s\n",
      "\n",
      "\n",
      "Logistic regression model MNIST test acc.:  0.5714\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 11.1961 | train acc.: 0.7308\n",
      "  E02 | train loss: 9.5207 | train acc.: 0.8846\n",
      "  E03 | train loss: 8.3357 | train acc.: 0.9231\n",
      "  E04 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "Logistic regression model training complete.\n",
      "\n",
      "Time to train logistic regression model:  0.4 s\n",
      "\n",
      "\n",
      "Logistic regression model MNIST test acc.:  0.4286\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 18.8475 | train acc.: 0.6296\n",
      "  E02 | train loss: 22.9300 | train acc.: 0.8148\n",
      "  E03 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "Logistic regression model training complete.\n",
      "\n",
      "Time to train logistic regression model:  0.3 s\n",
      "\n",
      "\n",
      "Logistic regression model MNIST test acc.:  0.5000\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 9.6785 | train acc.: 0.5556\n",
      "  E02 | train loss: 10.8170 | train acc.: 0.8889\n",
      "  E03 | train loss: 1.7063 | train acc.: 0.9630\n",
      "  E04 | train loss: 3.1037 | train acc.: 0.9259\n",
      "  E05 | train loss: 3.2468 | train acc.: 0.9630\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "Logistic regression model training complete.\n",
      "\n",
      "Time to train logistic regression model:  0.3 s\n",
      "\n",
      "\n",
      "Logistic regression model MNIST test acc.:  0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testkfold=[]\n",
    "nfold=5\n",
    "kfold = KFold(nfold,  True )\n",
    "for train, test in kfold.split(X_raw_train_npy_orig):\n",
    "#\tprint('train: %s, test: %s' % (X_raw_train_npy_orig[train].shape, X_raw_train_npy_orig[test].shape))\n",
    "    X_raw_train=X_raw_train_npy_orig[train]\n",
    "    X_raw_test=X_raw_train_npy_orig[test]\n",
    "    y_train=np.array(y_train_npy_orig[train] , dtype=float)\n",
    "    y_test=np.array(y_train_npy_orig[test], dtype=float)\n",
    "    #y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    #y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    X_raw_train = torch.from_numpy(X_raw_train) #format torch data\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_raw_test = torch.from_numpy(X_raw_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    X_raw_train = X_raw_train.float() #float\n",
    "    X_raw_test = X_raw_test.float()\n",
    "    \n",
    "    #X_train = X_raw_train / 255.0 #scale\n",
    "    #X_test = X_raw_test / 255.0\n",
    "    X_train=X_raw_train\n",
    "    X_test=X_raw_test\n",
    "    \n",
    "    \n",
    "    train_dataset = MNISTDataset(X_train, y_train) #pytorch data\n",
    "    test_dataset = MNISTDataset(X_test, y_test)\n",
    "    \n",
    "    batch_size = 10\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    input_dim = X_train.shape[1]             #Define model input and output dimensions from data\n",
    "    output_dim = len(np.unique(y_train.numpy()))  \n",
    "\n",
    "    \n",
    "    lr_model = LogisticRegression(input_dim=input_dim, output_dim=output_dim)# Instantiate logistic regression model\n",
    "    \n",
    "    lr = 0.1# Instantiate stochastic gradient descent (SGD) optimizer\n",
    "    \n",
    "    optimizer = torch.optim.SGD(lr_model.parameters(), lr=lr)\n",
    "    #optimizer = torch.optim.Adam(lr_model.parameters())\n",
    "    \n",
    "    print('\\nTraining logistic regression model...\\n')# Time model training\n",
    "    time_start = time.time()\n",
    "    \n",
    "    lr_model.train()# train() method affects operations such as dropout and batch normalization\n",
    "        # Train model/iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Initialize epoch metrics variables\n",
    "        n_obs = 0\n",
    "        loss_sum = 0\n",
    "        n_correct = 0\n",
    "\n",
    "        # Iterate through training data mini-batchesbatches\n",
    "        for images_batch, labels_batch in train_loader:\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = lr_model(images_batch)         # model predictions\n",
    "            loss = loss_func(y_pred, labels_batch.long())  # loss function evaluation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()    # backpropagation\n",
    "            optimizer.step()   # update parameters according to learning rate, gradients\n",
    "\n",
    "            # Update epoch metrics variables\n",
    "            n_batch = len(labels_batch)\n",
    "            n_obs += n_batch\n",
    "            loss_sum += n_batch * loss.item()\n",
    "            n_correct += (y_pred.argmax(1) == labels_batch.long()).float().sum().item()\n",
    "\n",
    "        # Calculate epoch training loss and training accuracy\n",
    "        loss_train = loss_sum / n_obs\n",
    "        acc_train = n_correct / n_obs\n",
    "\n",
    "        # Display training progress\n",
    "        prog_disp_freq = 1   # frequency of training progress display\n",
    "        if (epoch + 1) % prog_disp_freq == 0 or epoch == 0 or (epoch + 1) == n_epochs:\n",
    "            print('  E%02d | train loss: %s | train acc.: %s' % \n",
    "                  (epoch + 1, '{:.4f}'.format(loss_train), '{:.4f}'.format(acc_train)))\n",
    "\n",
    "    # Time model training\n",
    "    time_end = time.time()\n",
    "    print('\\nLogistic regression model training complete.\\n')\n",
    "    print('Time to train logistic regression model:  %.1f s\\n' % (time_end - time_start))\n",
    "        # Place model in evaluation mode\n",
    "    # .eval() method affects operations such as dropout and batch normalization\n",
    "    lr_model.eval()\n",
    "\n",
    "    # Initialize test set metrics variables\n",
    "    n_obs = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate through test data mini-batches\n",
    "        for images_batch, labels_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = lr_model(images_batch)  # model predictions\n",
    "\n",
    "            # Update test set metrics variables\n",
    "            n_batch = len(labels_batch)\n",
    "            n_obs += n_batch\n",
    "            n_correct += (y_pred.argmax(1) == labels_batch.long()).float().sum().item()\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    acc_test = n_correct / n_obs\n",
    "\n",
    "    # Display test accuracy\n",
    "    print('\\nLogistic regression model MNIST test acc.:  %.4f\\n' % (acc_test))\n",
    "    testkfold.append(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5360b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "249aa260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6047619047619046"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(testkfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54279888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8571428571428571,\n",
       " 0.5714285714285714,\n",
       " 0.42857142857142855,\n",
       " 0.5,\n",
       " 0.6666666666666666]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testkfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c36db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer perceptron model class definition\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a multi-layer perceptron model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Size/dimensionality of the model input data.\n",
    "    output_dim : int\n",
    "        Size/dimensionality of the model output.\n",
    "    hidden_dim : int\n",
    "        Size/dimensionality of the hidden layer.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Size/dimensionality of the model input data.\n",
    "    output_dim : int\n",
    "        Size/dimensionality of the model output.\n",
    "    hidden_dim : int\n",
    "        Size/dimensionality of the hidden layer.\n",
    "    linear_1 : torch.nn.Linear\n",
    "        Fully-connected/linear neural network layer.\n",
    "    linear_2 : torch.nn.Linear\n",
    "        Fully-connected/linear neural network layer.\n",
    "    relu : torch.nn.ReLU\n",
    "        Rectified linear unit activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # MLP instantiation method\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=50):\n",
    "        \"\"\"\n",
    "        Model instantiation method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Size/dimensionality of the model input data.\n",
    "        output_dim : int\n",
    "            Size/dimensionality of the model output.\n",
    "        hidden_dim : int\n",
    "            Size/dimensionality of the hidden layer.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Size/dimensionality of the model input data.\n",
    "        output_dim : int\n",
    "            Size/dimensionality of the model output.\n",
    "        hidden_dim : int\n",
    "            Size/dimensionality of the hidden layer.\n",
    "        linear_1 : torch.nn.Linear\n",
    "            Fully-connected/linear neural network layer.\n",
    "        linear_2 : torch.nn.Linear\n",
    "            Fully-connected/linear neural network layer.\n",
    "        relu : torch.nn.ReLU\n",
    "            Rectified linear unit activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inherit from torch.nn.Module\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Assign MLP parameters to model attributes\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define MLP layers\n",
    "        self.linear_1 = nn.Linear(\n",
    "            in_features=input_dim,\n",
    "            out_features=hidden_dim,\n",
    "            bias=True)\n",
    "        self.linear_2 = nn.Linear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=hidden_dim,\n",
    "            bias=True)\n",
    "        self.linear_3 = nn.Linear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=output_dim,\n",
    "            bias=True)\n",
    "        \n",
    "        # Define MLP activation function\n",
    "        self.relu = nn.LeakyReLU()\n",
    "    \n",
    "    # MLP forward pass method\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        MLP forward pass method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor()\n",
    "            Tensor of input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor()\n",
    "             Raw model predictions (not passed through sigmoid or softmax function).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Forward pass through model\n",
    "        x = self.linear_1(x)       # first fully-connected layer\n",
    "        x = self.relu(x)           # non-linear activation transformation\n",
    "        \n",
    "        s=self.linear_2(x)\n",
    "        s = self.relu(s)           # non-linear activation transformation\n",
    "\n",
    "        mids2=self.linear_2(s)\n",
    "        mids2 = self.relu(mids2)           # non-linear activation transformation\n",
    "        \n",
    "        mids3=self.linear_2(mids2)\n",
    "        mids3 = self.relu(mids3)           # non-linear activation transformation\n",
    "\n",
    "        \n",
    "        mids4=self.linear_2(mids3)\n",
    "        mids4 = self.relu(mids4)\n",
    "        \n",
    "        mids5=self.linear_2(mids4)\n",
    "        mids5 = self.relu(mids5)\n",
    "        \n",
    "        \n",
    "        logits = self.linear_3(s)  # second fully-connected layer\n",
    "        \n",
    "        # Return model output\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9a5ea2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15693399, -0.19200465,  1.24225113, ..., -0.52726602,\n",
       "         1.50944471,  1.94603694],\n",
       "       [-0.28760006,  1.33413896,  2.61789836, ...,  0.16872513,\n",
       "         3.50191173, -0.57236381],\n",
       "       [-0.95440114,  0.13828016, -0.68365498, ..., -0.52726602,\n",
       "        -0.48302231,  0.68683657],\n",
       "       ...,\n",
       "       [ 0.15693399, -0.86775978, -0.40852554, ...,  0.16872513,\n",
       "        -0.48302231, -0.57236381],\n",
       "       [ 0.2013874 , -0.74247933, -0.40852554, ...,  2.95268969,\n",
       "        -0.48302231, -0.57236381],\n",
       "       [-0.37650687, -0.25654306,  3.58085142, ...,  0.16872513,\n",
       "        -0.48302231, -0.57236381]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw_train_npy_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8915674b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa3f0468890>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(3)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22d05bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 0.5397 | train acc.: 0.7308\n",
      "  E02 | train loss: 0.0182 | train acc.: 1.0000\n",
      "  E03 | train loss: 0.0056 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "MLP model training complete.\n",
      "\n",
      "Time to train MLP model:  7.6 s\n",
      "\n",
      "\n",
      "MLP model MNIST test acc.:  0.7143\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 0.6897 | train acc.: 0.5769\n",
      "  E02 | train loss: 0.0114 | train acc.: 1.0000\n",
      "  E03 | train loss: 0.0069 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0002 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "MLP model training complete.\n",
      "\n",
      "Time to train MLP model:  8.7 s\n",
      "\n",
      "\n",
      "MLP model MNIST test acc.:  1.0000\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 0.6627 | train acc.: 0.5385\n",
      "  E02 | train loss: 0.1633 | train acc.: 0.9231\n",
      "  E03 | train loss: 0.0012 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0003 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0002 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "MLP model training complete.\n",
      "\n",
      "Time to train MLP model:  8.9 s\n",
      "\n",
      "\n",
      "MLP model MNIST test acc.:  1.0000\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 0.9788 | train acc.: 0.5185\n",
      "  E02 | train loss: 0.0798 | train acc.: 0.9630\n",
      "  E03 | train loss: 0.0432 | train acc.: 0.9630\n",
      "  E04 | train loss: 0.0229 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0040 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0011 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0007 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0002 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "MLP model training complete.\n",
      "\n",
      "Time to train MLP model:  8.6 s\n",
      "\n",
      "\n",
      "MLP model MNIST test acc.:  0.6667\n",
      "\n",
      "\n",
      "Training logistic regression model...\n",
      "\n",
      "  E01 | train loss: 0.6316 | train acc.: 0.5185\n",
      "  E02 | train loss: 0.0090 | train acc.: 1.0000\n",
      "  E03 | train loss: 0.0010 | train acc.: 1.0000\n",
      "  E04 | train loss: 0.0003 | train acc.: 1.0000\n",
      "  E05 | train loss: 0.0002 | train acc.: 1.0000\n",
      "  E06 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E07 | train loss: 0.0001 | train acc.: 1.0000\n",
      "  E08 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E09 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E10 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E11 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E12 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E13 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E14 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E15 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E16 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E17 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E18 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E19 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E20 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E21 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E22 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E23 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E24 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E25 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E26 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E27 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E28 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E29 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E30 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E31 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E32 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E33 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E34 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E35 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E36 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E37 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E38 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E39 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E40 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E41 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E42 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E43 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E44 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E45 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E46 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E47 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E48 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E49 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E50 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E51 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E52 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E53 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E54 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E55 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E56 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E57 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E58 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E59 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E60 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E61 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E62 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E63 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E64 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E65 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E66 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E67 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E68 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E69 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E70 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E71 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E72 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E73 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E74 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E75 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E76 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E77 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E78 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E79 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E80 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E81 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E82 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E83 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E84 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E85 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E86 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E87 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E88 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E89 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E90 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E91 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E92 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E93 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E94 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E95 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E96 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E97 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E98 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E99 | train loss: 0.0000 | train acc.: 1.0000\n",
      "  E100 | train loss: 0.0000 | train acc.: 1.0000\n",
      "\n",
      "MLP model training complete.\n",
      "\n",
      "Time to train MLP model:  6.9 s\n",
      "\n",
      "\n",
      "MLP model MNIST test acc.:  0.5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testkfold=[]\n",
    "nfold=5\n",
    "kfold = KFold(nfold,  True )\n",
    "for train, test in kfold.split(X_raw_train_npy_orig):\n",
    "#\tprint('train: %s, test: %s' % (X_raw_train_npy_orig[train].shape, X_raw_train_npy_orig[test].shape))\n",
    "    X_raw_train=X_raw_train_npy_orig[train]\n",
    "    X_raw_test=X_raw_train_npy_orig[test]\n",
    "    y_train=np.array(y_train_npy_orig[train] , dtype=float)\n",
    "    y_test=np.array(y_train_npy_orig[test], dtype=float)\n",
    "    \n",
    "    X_raw_train = torch.from_numpy(X_raw_train) #format torch data\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    X_raw_test = torch.from_numpy(X_raw_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    X_raw_train = X_raw_train.float() #float\n",
    "    X_raw_test = X_raw_test.float()\n",
    "    \n",
    "    #X_train = X_raw_train / 255.0 #scale\n",
    "    #X_test = X_raw_test / 255.0\n",
    "    X_train = X_raw_train \n",
    "    X_test = X_raw_test\n",
    "    \n",
    "    train_dataset = MNISTDataset(X_train, y_train) #pytorch data\n",
    "    test_dataset = MNISTDataset(X_test, y_test)\n",
    "    \n",
    "    batch_size = 10\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "        # Define model input and output dimensions from data\n",
    "    input_dim = X_train.shape[1]                  # second dimension is data dimensionality\n",
    "    output_dim = len(np.unique(y_train.numpy()))  # output dim. is equal to number of unique labels\n",
    "\n",
    "    # Instantiate multi-layer perceptron model\n",
    "    mlp_model = MLP(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "    # Instantiate stochastic gradient descent (SGD) optimizer\n",
    "    lr = 0.1\n",
    "    #optimizer = torch.optim.SGD(mlp_model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters())\n",
    "\n",
    "    \n",
    "    print('\\nTraining logistic regression model...\\n')# Time model training\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Place model in training mode\n",
    "    # .train() method affects operations such as dropout and batch normalization\n",
    "    mlp_model.train()\n",
    "\n",
    "    # Train model/iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Initialize epoch metrics variables\n",
    "        n_obs = 0\n",
    "        loss_sum = 0\n",
    "        n_correct = 0\n",
    "\n",
    "        # Iterate through training data mini-batches\n",
    "        for images_batch, labels_batch in train_loader:\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = mlp_model(images_batch)         # model predictions\n",
    "            loss = loss_func(y_pred, labels_batch.long())  # loss function evaluation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()    # backpropagation\n",
    "            optimizer.step()   # update parameters according to learning rate, gradients\n",
    "\n",
    "            # Update epoch metrics variables\n",
    "            n_batch = len(labels_batch)\n",
    "            n_obs += n_batch\n",
    "            loss_sum += n_batch * loss.item()\n",
    "            n_correct += (y_pred.argmax(1) == labels_batch.long()).float().sum().item()\n",
    "\n",
    "        # Calculate epoch training loss and training accuracy\n",
    "        loss_train = loss_sum / n_obs\n",
    "        acc_train = n_correct / n_obs\n",
    "\n",
    "        # Display training progress\n",
    "        prog_disp_freq = 1   # frequency of training progress display\n",
    "        if (epoch + 1) % prog_disp_freq == 0 or epoch == 0 or epoch + 1 == n_epochs:\n",
    "            print('  E%02d | train loss: %s | train acc.: %s' % \n",
    "                  (epoch + 1, '{:.4f}'.format(loss_train), '{:.4f}'.format(acc_train)))\n",
    "\n",
    "    # Time model training\n",
    "    time_end = time.time()\n",
    "    print('\\nMLP model training complete.\\n')\n",
    "    print('Time to train MLP model:  %.1f s\\n' % (time_end - time_start))\n",
    "    # Place model in evaluation mode\n",
    "    # .eval() method affects operations such as dropout and batch normalization\n",
    "    mlp_model.eval()\n",
    "\n",
    "    # Initialize test set metrics variables\n",
    "    n_obs = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Iterate through test data mini-batches\n",
    "        for images_batch, labels_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = mlp_model(images_batch)  # model predictions\n",
    "\n",
    "            # Update test set metrics variables\n",
    "            n_batch = len(labels_batch)\n",
    "            n_obs += n_batch\n",
    "            n_correct += (y_pred.argmax(1) == labels_batch.long()).float().sum().item()\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    acc_test = n_correct / n_obs\n",
    "\n",
    "    # Display test accuracy\n",
    "    print('\\nMLP model MNIST test acc.:  %.4f\\n' % (acc_test))\n",
    "    testkfold.append(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9f8a657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761904761904762"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(testkfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "819bfcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7142857142857143, 1.0, 1.0, 0.6666666666666666, 0.5]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testkfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d213b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1569, -0.1920,  1.2423,  ..., -0.5273,  1.5094,  1.9460],\n",
       "        [ 0.2236,  0.4040, -0.4085,  ..., -0.5273,  1.5094,  1.9460],\n",
       "        [-0.5543, -0.1351, -0.1334,  ...,  0.8647, -0.4830, -0.5724],\n",
       "        [ 0.2236,  2.1769,  0.4169,  ..., -0.5273, -0.4830, -0.5724],\n",
       "        [ 0.2458,  0.1003, -0.4085,  ...,  0.1687, -0.4830, -0.5724],\n",
       "        [ 0.2014, -0.7425, -0.4085,  ...,  2.9527, -0.4830, -0.5724]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42be92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
